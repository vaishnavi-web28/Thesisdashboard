# -*- coding: utf-8 -*-
"""Dashboard1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v7LUjJJAimZAXnz-cAzJsnuZU0tjTRRv
"""

# â”€â”€â”€ GOOGLE COLAB SETUP: Mount Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from google.colab import drive
drive.mount('/content/drive')

# â”€â”€â”€ STREAMLIT IMPORTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from transformers import pipeline
from textwrap import shorten
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from io import BytesIO

# â”€â”€â”€ Download VADER lexicon â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
nltk.download('vader_lexicon', quiet=True)
nltk.download('stopwords', quiet=True)
sia = SentimentIntensityAnalyzer()

# â”€â”€â”€ Summarizer Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def get_summarizer():
    return pipeline(
        "summarization",
        model="sshleifer/distilbart-cnn-12-6",
        device=-1
    )

summarizer = get_summarizer()

# â”€â”€â”€ Load Data from Google Drive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_and_clean():
    # ğŸ” Change the path to your actual file location in your Drive
    file_path = '/content/drive/MyDrive/software_reviews_100000.csv'
    df = pd.read_csv(file_path)

    df = df.dropna(subset=['text', 'rating', 'timestamp'])
    df['rating'] = pd.to_numeric(df['rating'], errors='coerce').clip(1,5)
    df = df.dropna(subset=['rating'])
    df['review_date'] = pd.to_datetime(df['timestamp'], unit='ms', errors='coerce')
    df = df.dropna(subset=['review_date']).reset_index(drop=True)
    return df

df = load_and_clean()

# â”€â”€â”€ Sidebar Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.markdown(f"**Reviews after cleaning:** {df.shape[0]:,}")

# â”€â”€â”€ 1. Rating Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("1. Rating Distribution")
rating_counts = df['rating'].value_counts().sort_index()
df_rating = rating_counts.rename_axis('rating').reset_index(name='count')
fig1 = px.bar(df_rating, x='rating', y='count', labels={'rating':'Star Rating','count':'Count'},
              title="Number of Reviews vs Ratings")
st.plotly_chart(fig1, use_container_width=True)

# â”€â”€â”€ 2. Sentiment Polarity Histogram â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("2. Sentiment Polarity Histogram")
df['polarity'] = df['text'].apply(lambda t: sia.polarity_scores(str(t))['compound'])

def sentiment_cat(p):
    if p <= -0.6: return "Very Negative"
    if p <= -0.2: return "Negative"
    if p <   0.2: return "Neutral"
    if p <   0.6: return "Positive"
    return "Very Positive"

df['sentiment_cat'] = df['polarity'].apply(sentiment_cat)

fig2 = px.histogram(df, x='polarity', nbins=50, title="Distribution of Review Polarity (VADER Compound Score)")
st.plotly_chart(fig2, use_container_width=True)

# â”€â”€â”€ 3. Top Positive & Negative Examples â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("3. Top Positive & Negative Examples")
if len(df) >= 10:
    top_pos = df.nlargest(5, 'polarity')
    top_neg = df.nsmallest(5, 'polarity')

    def summarize(text):
        try:
            return summarizer(str(text), max_length=50, min_length=10, do_sample=False)[0]['summary_text']
        except:
            return shorten(str(text), width=100, placeholder="â€¦")

    st.subheader("Most Positive Reviews")
    pos_cols = st.columns(5)
    for (idx, row), col in zip(top_pos.iterrows(), pos_cols):
        with col:
            st.markdown(f"**Rating:** {row['rating']}  |  **Polarity:** {row['polarity']:.2f}")
            st.write(shorten(row['text'], width=150, placeholder="â€¦"))
            st.write(f"ğŸ” **Summary:** {summarize(row['text'])}")

    st.markdown("---")
    st.subheader("Most Negative Reviews")
    neg_cols = st.columns(5)
    for (idx, row), col in zip(top_neg.iterrows(), neg_cols):
        with col:
            st.markdown(f"**Rating:** {row['rating']}  |  **Polarity:** {row['polarity']:.2f}")
            st.write(shorten(row['text'], width=150, placeholder="â€¦"))
            st.write(f"ğŸ” **Summary:** {summarize(row['text'])}")
else:
    st.warning("Not enough data to show top examples.")

# â”€â”€â”€ 4. Treemap Visualization â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("4. Rating â†’ Sentiment Hierarchy (Treemap)")
matrix = df.groupby(['rating','sentiment_cat']).size().reset_index(name='count')
matrix['rating'] = pd.Categorical(matrix['rating'], categories=[1,2,3,4,5], ordered=True)
matrix['sentiment_cat'] = pd.Categorical(matrix['sentiment_cat'],
    categories=["Very Negative","Negative","Neutral","Positive","Very Positive"], ordered=True)
matrix = matrix.sort_values(['rating','sentiment_cat'])

fig_treemap = px.treemap(
    matrix,
    path=[px.Constant("All Reviews"), "rating", "sentiment_cat"],
    values="count",
    color="count",
    color_continuous_scale="Blues",
    title="Reviews by Star Rating and Sentiment Category"
)
fig_treemap.update_traces(root_color="lightgrey", textinfo="label+value", tiling=dict(pad=4))
st.plotly_chart(fig_treemap, use_container_width=True)

# â”€â”€â”€ 5. Time-based Sentiment Heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("5. Average Sentiment by Day of Week & Hour")
df['day_of_week'] = df['review_date'].dt.day_name()
df['hour'] = df['review_date'].dt.hour

pivot = df.groupby(['hour','day_of_week'])['polarity'].mean().reset_index()
days = ["Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"]
pivot['day_of_week'] = pd.Categorical(pivot['day_of_week'], categories=days, ordered=True)
heat_df = pivot.pivot(index='hour', columns='day_of_week', values='polarity')

fig_dh = px.imshow(
    heat_df,
    labels={'x':'Day of Week','y':'Hour of Day','color':'Avg Polarity'},
    x=heat_df.columns,
    y=heat_df.index,
    aspect='auto',
    color_continuous_scale='RdYlBu_r',
    origin='lower',
    title="When Are Reviews Most Positive or Negative?"
)
fig_dh.update_layout(yaxis=dict(dtick=1))
st.plotly_chart(fig_dh, use_container_width=True)

# â”€â”€â”€ 6. Word Cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.header("6. Top 50 Word Frequencies (Word Cloud)")
texts = df['text'].dropna().str.lower().tolist()
all_text = " ".join(texts)
wc = WordCloud(
    width=800, height=400,
    background_color='white',
    max_words=50,
    stopwords=set(nltk.corpus.stopwords.words('english'))
).generate(all_text)

buf = BytesIO()
wc.to_image().save(buf, format='PNG')
st.image(buf.getvalue(), use_column_width=True, caption="Most Common Words in Reviews")

